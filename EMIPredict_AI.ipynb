{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a823235",
   "metadata": {},
   "source": [
    "# EMIPredict_AI\n",
    "\n",
    "**Author:** Mahima Patel\n",
    "\n",
    "**Objective:** Build a complete pipeline to predict EMI eligibility (classification) and maximum monthly EMI (regression). This notebook includes data cleaning, exploratory data analysis (15+ charts), hypothesis testing (T-test, Chi-square, Pearson correlation, ANOVA), feature engineering, model training (Logistic Regression, RandomForest, XGBoost for classification; Linear Regression, RandomForestRegressor, XGBoostRegressor for regression), and saving artifacts for deployment.\n",
    "\n",
    "**Notes:**\n",
    "- Designed to run locally or in Google Colab.\n",
    "- Uses a sampling option (`SAMPLE_ROWS`) to speed up processing on large datasets.\n",
    "- Outputs are saved under `artifacts/`:\n",
    "  - `artifacts/eda_charts/` — PNG charts\n",
    "  - `artifacts/models/` — saved joblib models and scaler\n",
    "  - `artifacts/reports/` — EDA report and performance reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Settings\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import joblib\n",
    "\n",
    "# plotting defaults\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (9,5)\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "# Artifact folders\n",
    "os.makedirs('artifacts/eda_charts', exist_ok=True)\n",
    "os.makedirs('artifacts/models', exist_ok=True)\n",
    "os.makedirs('artifacts/reports', exist_ok=True)\n",
    "\n",
    "# User-editable settings\n",
    "DATA_PATH = 'emi_prediction_dataset.csv'  # change if your file name differs\n",
    "SAMPLE_ROWS = 50000  # set to None to use full dataset (careful with large files)\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (with optional sampling)\n",
    "print('Loading dataset from:', DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print('Original shape:', df.shape)\n",
    "\n",
    "if SAMPLE_ROWS and df.shape[0] > SAMPLE_ROWS:\n",
    "    df = df.sample(n=SAMPLE_ROWS, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    print('Sampled shape:', df.shape)\n",
    "\n",
    "# Quick preview\n",
    "display(df.head())\n",
    "print('\\nColumn data types:')\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coerce numeric-like columns and detect categorical columns\n",
    "numeric_cols = []\n",
    "for c in df.columns:\n",
    "    coerced = pd.to_numeric(df[c], errors='coerce')\n",
    "    if coerced.notnull().mean() > 0.7:\n",
    "        df[c] = coerced\n",
    "        numeric_cols.append(c)\n",
    "cat_cols = [c for c in df.columns if c not in numeric_cols]\n",
    "\n",
    "print('Detected numeric columns (sample):', numeric_cols[:30])\n",
    "print('Detected categorical columns (sample):', cat_cols[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fabbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values & basic cleaning\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "if numeric_cols:\n",
    "    df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "if cat_cols:\n",
    "    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "\n",
    "# Drop exact duplicates\n",
    "before = df.shape[0]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "after = df.shape[0]\n",
    "print(f'Dropped {before-after} duplicate rows. New shape: {df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "# Create debt_to_income, total_monthly_expenses, expense_to_income when possible\n",
    "if set(['monthly_salary','current_emi_amount']).issubset(df.columns):\n",
    "    df['debt_to_income'] = df['current_emi_amount'] / (df['monthly_salary'] + 1e-6)\n",
    "\n",
    "if set(['current_emi_amount','other_monthly_expenses']).issubset(df.columns):\n",
    "    df['total_monthly_expenses'] = df[['current_emi_amount','other_monthly_expenses']].sum(axis=1)\n",
    "\n",
    "if 'total_monthly_expenses' in df.columns and 'monthly_salary' in df.columns:\n",
    "    df['expense_to_income'] = df['total_monthly_expenses'] / (df['monthly_salary'] + 1e-6)\n",
    "\n",
    "# Create credit buckets if credit_score exists\n",
    "if 'credit_score' in df.columns:\n",
    "    bins=[300,500,650,700,750,850]\n",
    "    df['credit_bucket'] = pd.cut(df['credit_score'], bins=bins, include_lowest=True)\n",
    "\n",
    "print('Feature engineering complete. New columns (if created):', [c for c in ['debt_to_income','total_monthly_expenses','expense_to_income','credit_bucket'] if c in df.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical columns with LabelEncoder (for modeling)\n",
    "label_encoders = {}\n",
    "for c in cat_cols:\n",
    "    try:\n",
    "        le = LabelEncoder()\n",
    "        df[c] = le.fit_transform(df[c].astype(str))\n",
    "        label_encoders[c] = le\n",
    "    except Exception as e:\n",
    "        print('Could not encode', c, e)\n",
    "\n",
    "# Save encoders\n",
    "joblib.dump(label_encoders, 'artifacts/models/label_encoders.joblib')\n",
    "print('Saved label encoders to artifacts/models/label_encoders.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c931b2",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "We will create 15+ charts and save them into `artifacts/eda_charts/`. Each chart will have a short interpretation hint in the notebook for you to copy into the final report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Generate charts (>=15)\n",
    "charts = []\n",
    "\n",
    "def save_fig(fig, name):\n",
    "    path = os.path.join('artifacts/eda_charts', name)\n",
    "    fig.tight_layout(); fig.savefig(path); plt.close(fig)\n",
    "    charts.append(path)\n",
    "\n",
    "# 1-8: histograms for top numeric features\n",
    "top_nums = numeric_cols[:8]\n",
    "for c in top_nums:\n",
    "    fig = plt.figure()\n",
    "    plt.hist(df[c].dropna(), bins=30)\n",
    "    plt.title(f'Distribution: {c}')\n",
    "    plt.xlabel(c); plt.ylabel('count')\n",
    "    save_fig(fig, f'chart_hist_{c}.png')\n",
    "\n",
    "# 9: EMI scenario distribution (bar)\n",
    "if 'emi_scenario' in df.columns:\n",
    "    fig = plt.figure()\n",
    "    vals = df['emi_scenario'].value_counts()\n",
    "    plt.bar(vals.index.astype(str), vals.values)\n",
    "    plt.title('EMI scenario distribution')\n",
    "    save_fig(fig, 'chart_emi_scenario.png')\n",
    "\n",
    "# 10: monthly_salary histogram\n",
    "if 'monthly_salary' in df.columns:\n",
    "    fig = plt.figure(); plt.hist(df['monthly_salary'], bins=40)\n",
    "    plt.title('Monthly salary distribution'); save_fig(fig,'chart_salary.png')\n",
    "\n",
    "# 11: requested_amount by emi_scenario (boxplot)\n",
    "if 'requested_amount' in df.columns and 'emi_scenario' in df.columns:\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    uniq = df['emi_scenario'].unique()[:10]\n",
    "    groups = [df[df['emi_scenario']==g]['requested_amount'].values for g in uniq]\n",
    "    plt.boxplot(groups, labels=[str(g) for g in uniq])\n",
    "    plt.title('Requested amount by EMI scenario (top 10)')\n",
    "    save_fig(fig,'chart_req_by_scenario.png')\n",
    "\n",
    "# 12: salary vs max_monthly_emi scatter\n",
    "if 'monthly_salary' in df.columns and 'max_monthly_emi' in df.columns:\n",
    "    fig = plt.figure(); plt.scatter(df['monthly_salary'], df['max_monthly_emi'], alpha=0.3)\n",
    "    plt.title('Salary vs max_monthly_emi'); save_fig(fig,'chart_salary_vs_emi.png')\n",
    "\n",
    "# 13: correlation heatmap\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "corr = df.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation matrix (numeric features)')\n",
    "save_fig(fig,'chart_corr.png')\n",
    "\n",
    "# 14-15: debt_to_income and expense_to_income histograms\n",
    "for col in ['debt_to_income','expense_to_income']:\n",
    "    if col in df.columns:\n",
    "        fig = plt.figure(); plt.hist(df[col].dropna(), bins=30); plt.title(col); save_fig(fig,f'chart_{col}.png')\n",
    "\n",
    "# 16: credit_bucket counts\n",
    "if 'credit_bucket' in df.columns:\n",
    "    fig = plt.figure(); vals = df['credit_bucket'].value_counts().sort_index(); plt.bar([str(i) for i in vals.index], vals.values); plt.title('Credit buckets'); save_fig(fig,'chart_credit_buckets.png')\n",
    "\n",
    "# 17: gender pie\n",
    "if 'gender' in df.columns:\n",
    "    fig = plt.figure(); vals = df['gender'].value_counts(); plt.pie(vals.values, labels=[str(i) for i in vals.index], autopct='%1.1f%%'); plt.title('Gender distribution'); save_fig(fig,'chart_gender.png')\n",
    "\n",
    "# 18: years_of_employment vs avg max_monthly_emi\n",
    "if 'years_of_employment' in df.columns and 'max_monthly_emi' in df.columns:\n",
    "    grp = df.groupby('years_of_employment')['max_monthly_emi'].mean().reset_index()\n",
    "    fig = plt.figure(); plt.plot(grp['years_of_employment'], grp['max_monthly_emi'], marker='o'); plt.title('Avg max_monthly_emi by years_of_employment'); save_fig(fig,'chart_years_vs_emi.png')\n",
    "\n",
    "# 19: dependents vs eligibility stacked bar\n",
    "if 'dependents' in df.columns and 'emi_eligibility' in df.columns:\n",
    "    cross = pd.crosstab(df['dependents'], df['emi_eligibility'])\n",
    "    fig = plt.figure(); cross.plot(kind='bar', stacked=True); plt.title('Dependents vs EMI eligibility'); plt.tight_layout(); save_fig(fig,'chart_dependents_vs_eligibility.png')\n",
    "\n",
    "print('Charts saved:', len(charts))\n",
    "print('\\nSample saved chart paths:')\n",
    "for p in charts[:10]:\n",
    "    print('-', p)\n",
    "\n",
    "# save simple EDA report file\n",
    "with open('artifacts/reports/EDA_report.md','w') as f:\n",
    "    f.write('# EDA Report\\n\\n')\n",
    "    f.write('Charts generated:\\n')\n",
    "    for p in charts:\n",
    "        f.write(f'- {p}\\n')\n",
    "    f.write('\\n\\nInterpretations: add your observations under each chart in this file.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c2c28",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "We perform 4 hypothesis tests with clear null and alternative hypotheses:\n",
    "\n",
    "1. **T-test**: Mean monthly salary differs between eligible and not-eligible applicants.\n",
    "2. **Chi-square**: Gender and EMI eligibility are dependent.\n",
    "3. **Pearson correlation**: Credit score correlates with max_monthly_emi.\n",
    "4. **ANOVA**: Mean debt_to_income differs across employment types.\n",
    "\n",
    "Each test outputs the test statistic, p-value, and interpretation (reject/fail to reject H0 at α=0.05).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cccdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing\n",
    "ALPHA = 0.05\n",
    "\n",
    "# 1: T-test (salary between eligible and not eligible)\n",
    "if 'monthly_salary' in df.columns and 'emi_eligibility' in df.columns:\n",
    "    # try to treat emi_eligibility as binary (0/1) or encoded\n",
    "    unique_vals = sorted(df['emi_eligibility'].unique())\n",
    "    if len(unique_vals) >= 2:\n",
    "        # pick first two groups for comparison\n",
    "        g1 = df[df['emi_eligibility'] == unique_vals[0]]['monthly_salary']\n",
    "        g2 = df[df['emi_eligibility'] == unique_vals[-1]]['monthly_salary']\n",
    "        tstat, pval = stats.ttest_ind(g1.dropna(), g2.dropna(), equal_var=False)\n",
    "        print('T-test: monthly_salary between groups', unique_vals[0], 'and', unique_vals[-1])\n",
    "        print('t-statistic =', round(tstat,4), ', p-value =', round(pval,6))\n",
    "        if pval < ALPHA:\n",
    "            print('Result: Reject H0 -> mean salaries differ (p <', ALPHA,')')\n",
    "        else:\n",
    "            print('Result: Fail to reject H0 -> no evidence of difference (p >=', ALPHA,')')\n",
    "\n",
    "# 2: Chi-square (gender vs eligibility)\n",
    "if 'gender' in df.columns and 'emi_eligibility' in df.columns:\n",
    "    ct = pd.crosstab(df['gender'], df['emi_eligibility'])\n",
    "    chi2, p, dof, ex = stats.chi2_contingency(ct)\n",
    "    print('\\nChi-square test: gender vs emi_eligibility')\n",
    "    print('chi2 =', round(chi2,4), ', p-value =', round(p,6))\n",
    "    if p < ALPHA:\n",
    "        print('Result: Reject H0 -> gender and eligibility are dependent')\n",
    "    else:\n",
    "        print('Result: Fail to reject H0 -> no evidence of dependence')\n",
    "\n",
    "# 3: Pearson correlation (credit_score vs max_monthly_emi)\n",
    "if 'credit_score' in df.columns and 'max_monthly_emi' in df.columns:\n",
    "    r, p = stats.pearsonr(df['credit_score'].fillna(0), df['max_monthly_emi'].fillna(0))\n",
    "    print('\\nPearson correlation: credit_score vs max_monthly_emi')\n",
    "    print('r =', round(r,4), ', p-value =', round(p,6))\n",
    "    if p < ALPHA:\n",
    "        print('Result: Reject H0 -> significant correlation exists (p <', ALPHA,')')\n",
    "    else:\n",
    "        print('Result: Fail to reject H0 -> no significant correlation')\n",
    "\n",
    "# 4: ANOVA (debt_to_income across employment_type)\n",
    "if 'debt_to_income' in df.columns and 'employment_type' in df.columns:\n",
    "    groups = [g['debt_to_income'].values for n,g in df.groupby('employment_type') if len(g) > 5]\n",
    "    if len(groups) > 1:\n",
    "        fstat, p = stats.f_oneway(*groups)\n",
    "        print('\\nANOVA: debt_to_income across employment_type groups')\n",
    "        print('F =', round(fstat,4), ', p-value =', round(p,6))\n",
    "        if p < ALPHA:\n",
    "            print('Result: Reject H0 -> at least one group mean differs')\n",
    "        else:\n",
    "            print('Result: Fail to reject H0 -> no evidence of differences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00362684",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare features for modeling\n",
    "# Choose explicit targets (if present) and prepare numeric features\n",
    "class_col = 'emi_eligibility' if 'emi_eligibility' in df.columns else None\n",
    "reg_col = 'max_monthly_emi' if 'max_monthly_emi' in df.columns else None\n",
    "\n",
    "# Select numeric features only for a simple baseline pipeline\n",
    "features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# remove targets\n",
    "for t in [class_col, reg_col]:\n",
    "    if t in features:\n",
    "        features.remove(t)\n",
    "\n",
    "X = df[features].fillna(0)\n",
    "print('Number of numeric features used:', X.shape[1])\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "joblib.dump(scaler, 'artifacts/models/scaler.joblib')\n",
    "print('Saved scaler to artifacts/models/scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cefefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification models (EMI eligibility)\n",
    "if class_col:\n",
    "    y = df[class_col].astype(int)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(Xs, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "    class_models = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', n_estimators=100, random_state=RANDOM_STATE)\n",
    "    }\n",
    "    class_results = []\n",
    "    for name, m in class_models.items():\n",
    "        print('\\nTraining', name)\n",
    "        m.fit(Xtr, ytr)\n",
    "        preds = m.predict(Xte)\n",
    "        probs = None\n",
    "        try:\n",
    "            probs = m.predict_proba(Xte)\n",
    "        except:\n",
    "            probs = None\n",
    "        acc = accuracy_score(yte, preds)\n",
    "        prec = precision_score(yte, preds, average='weighted', zero_division=0)\n",
    "        rec = recall_score(yte, preds, average='weighted', zero_division=0)\n",
    "        f1s = f1_score(yte, preds, average='weighted', zero_division=0)\n",
    "        roc = None\n",
    "        if probs is not None and probs.shape[1] > 1:\n",
    "            try:\n",
    "                roc = roc_auc_score(pd.get_dummies(yte), probs, average='weighted', multi_class='ovr')\n",
    "            except:\n",
    "                roc = None\n",
    "        class_results.append({'model':name,'accuracy':acc,'precision':prec,'recall':rec,'f1':f1s,'roc_auc':roc})\n",
    "        joblib.dump(m, f'artifacts/models/{name}_classification.joblib')\n",
    "    class_df = pd.DataFrame(class_results)\n",
    "    display(class_df)\n",
    "    class_df.to_excel('artifacts/reports/classification_performance.xlsx', index=False)\n",
    "    print('Saved classification performance to artifacts/reports/classification_performance.xlsx')\n",
    "else:\n",
    "    print('No classification target found (emi_eligibility)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regression models (Max monthly EMI)\n",
    "if reg_col:\n",
    "    y = df[reg_col].astype(float)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(Xs, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    reg_models = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'RandomForestRegressor': RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE),\n",
    "        'XGBoostRegressor': XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=RANDOM_STATE)\n",
    "    }\n",
    "    reg_results = []\n",
    "    for name, m in reg_models.items():\n",
    "        print('\\nTraining', name)\n",
    "        m.fit(Xtr, ytr)\n",
    "        preds = m.predict(Xte)\n",
    "        rmse = np.sqrt(mean_squared_error(yte, preds))\n",
    "        mae = mean_absolute_error(yte, preds)\n",
    "        r2 = r2_score(yte, preds)\n",
    "        mape = (np.mean(np.abs((yte - preds) / (yte + 1e-6)))) * 100\n",
    "        reg_results.append({'model':name,'rmse':rmse,'mae':mae,'r2':r2,'mape':mape})\n",
    "        joblib.dump(m, f'artifacts/models/{name}_regression.joblib')\n",
    "    reg_df = pd.DataFrame(reg_results)\n",
    "    display(reg_df)\n",
    "    reg_df.to_excel('artifacts/reports/regression_performance.xlsx', index=False)\n",
    "    print('Saved regression performance to artifacts/reports/regression_performance.xlsx')\n",
    "else:\n",
    "    print('No regression target found (max_monthly_emi)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final artifacts list\n",
    "import glob\n",
    "artifacts = glob.glob('artifacts/**', recursive=True)\n",
    "print('Artifacts (selected):')\n",
    "for a in artifacts:\n",
    "    if a.count('/') <= 2: continue\n",
    "    print('-', a)\n",
    "\n",
    "print('\\nNotebook run complete. Check artifacts/ for charts, models, and reports.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89e184",
   "metadata": {},
   "source": [
    "## Final notes & next steps\n",
    "- This notebook provides a full baseline pipeline. For a high-scoring submission, add detailed interpretations under each EDA chart in `artifacts/reports/EDA_report.md`.\n",
    "- Next improvements (recommended):\n",
    "  - Hyperparameter tuning with `GridSearchCV` or `Optuna`.\n",
    "  - Use MLflow to log experiments and compare runs.\n",
    "  - Build a REST API with FastAPI for model serving.\n",
    "  - Add SHAP or permutation feature importance for explainability.\n",
    "\n",
    "**Deliverables produced by this notebook:**\n",
    "- `artifacts/eda_charts/` (>=15 PNGs)\n",
    "- `artifacts/models/` (.joblib models + scaler + encoders)\n",
    "- `artifacts/reports/` (performance Excel files and EDA report)\n",
    "\n",
    "Good luck — want me to run a sample execution here to generate artifacts (50k sample)?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
